# 集群脑裂-参数配置

### 集群脑裂的问题

### 什么是集群脑裂

es 在主节点上产生分歧，产生多个master 节点，从而使集群分裂成多个同名集群，使得集群处于异常状态。

当出现多个master节点的时候，可能发生写入请求分配到不同的master节点，而数据只保存在对应的master节点的分片上，不会复制到其它节点。**此时若访问不同的节点，会发现查询的结果是不一样的。**

### 举例说明脑裂

`discovery.zen.minimum_master_nodes` 参数之前设置为 1（默认值）。

这个参数的含义是限制选举master节点的数量。

- 当master节点不存在时，至少有几个master候选节点才可以开始选举master节点。
- 当集群内候选节点数量不足时，限制取消master节点。

---

因为我们部署的 es 是3个节点，默认第1个es节点作为master节点。假如 master 节点和其余两个data节点之前网络不通，就会出现两种情况。

- master 节点认为集群中只存在它这一个节点，而且满足候选节点数量1，所以master节点还是master节点。
- 两个data节点之间通信正常，认为集群中存在两个节点，同样满足候选节点数量1，所以两个data节点会选举一个master节点出来。

此时整个es集群有两个master节点，出现了脑裂的情况。

### 解决脑裂的方式

**master候选资格节点数量 / 2 + 1**，所有有资格成为master的节点都需要加上这个配置。

即 `discovery.zen.minimum_master_nodes` 参数设置为 2 。

![image-20250718175531334](https://s2.loli.net/2025/07/18/QLTbwayPGnfV95O.png)

假如 master 节点和其余两个data节点之前网络不通，就会出现两种情况。

- master 节点认为集群中只存在它这一个节点，不满足候选节点数量2，此时master节点会变更为data节点。
- 两个data节点之间通信正常，认为集群中存在两个节点，满足配置候选节点数量2，所以两个data节点会选举一个master节点出来。

此时整个es集群只有1个master节点，不会出现脑裂情况。